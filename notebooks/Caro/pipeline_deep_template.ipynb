{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "Test.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": true
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-02-26T15:33:58.624633Z",
          "start_time": "2021-02-26T15:33:58.609857Z"
        },
        "id": "protective-sequence"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "id": "protective-sequence",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-02-26T15:34:00.366476Z",
          "start_time": "2021-02-26T15:33:58.960399Z"
        },
        "id": "naughty-representative"
      },
      "source": [
        "import pandas as pd"
      ],
      "id": "naughty-representative",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XRZRFrfS-Ps",
        "outputId": "86e023f7-2a3b-4aff-c755-25672f0d5841"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "id": "_XRZRFrfS-Ps",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSbBE7d_S9cL"
      },
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/bitstampUSD.csv')"
      ],
      "id": "CSbBE7d_S9cL",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uh7q5eVUTUuA"
      },
      "source": [
        "data = data[2798176:4727776].copy()"
      ],
      "id": "Uh7q5eVUTUuA",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sd2JZ4K6Bzc3"
      },
      "source": [
        "def preprocessing_data(data, features_size, h):\r\n",
        "        \r\n",
        "    data_pp = data.copy()\r\n",
        "    data_pp['diff_Open'] = data_pp['Open'].diff(h)\r\n",
        "    data_pp['diff_Open'] = data_pp['diff_Open'].dropna()\r\n",
        "    data_pp[f\"t+{h}\"] = data_pp['diff_Open'].shift(-h)\r\n",
        "    \r\n",
        "    for i in range(0, features_size):\r\n",
        "        data_pp[f't-{i}'] = data_pp['Open'].shift(i)\r\n",
        "    data_shifted = data_pp.dropna()\r\n",
        "    \r\n",
        "    return data_shifted\r\n"
      ],
      "id": "Sd2JZ4K6Bzc3",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "parallel-satellite"
      },
      "source": [
        "def features_target(data_shifted, h):\r\n",
        "    \r\n",
        "    X = data_shifted.drop(columns=['Open', 'diff_Open', f\"t+{h}\"])\r\n",
        "    y = data_shifted[f\"t+{h}\"].copy()\r\n",
        "    y[y > 0] = 1\r\n",
        "    y[y <= 0] = 0\r\n",
        "    \r\n",
        "    data_size = data_shifted.shape[0]\r\n",
        "    \r\n",
        "    return X, y, data_size"
      ],
      "id": "parallel-satellite",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gZgCS0zBwym"
      },
      "source": [
        "def select_date(data, date_start, date_end):\r\n",
        "    data['Timestamp'] = pd.to_datetime(data['Timestamp'], unit='s', origin='unix')\r\n",
        "    data = data[['Open', 'Timestamp']].set_index(\"Timestamp\").fillna(method='ffill')\r\n",
        "\r\n",
        "    if date_start != None:\r\n",
        "        if date_end != None:\r\n",
        "            data = data[date_start:date_end].copy()\r\n",
        "    else:\r\n",
        "        data = data.copy()\r\n",
        "        \r\n",
        "    return data"
      ],
      "id": "-gZgCS0zBwym",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87kspYxEyAQ7"
      },
      "source": [
        "def input_data(X, y, sample_size, data_size, train_size, test_size, h=1, w=0):    \r\n",
        " \r\n",
        "\r\n",
        "    sample_X = X.iloc[data_size-(test_size * w + sample_size) : data_size - (test_size * w)]\r\n",
        "    sample_y = y.iloc[data_size-(test_size * w + sample_size) : data_size - (test_size * w)]\r\n",
        "    \r\n",
        "    X_train = sample_X.iloc[0:train_size]\r\n",
        "    y_train = sample_y.iloc[0:train_size]\r\n",
        "    X_test = sample_X.iloc[(train_size+h-1):(sample_size)]\r\n",
        "    y_test = sample_y.iloc[(train_size+h-1):(sample_size)]\r\n",
        "    \r\n",
        "    return X_train, X_test, y_train, y_test"
      ],
      "id": "87kspYxEyAQ7",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WozBM1xn0cb1"
      },
      "source": [
        "import numpy as np\r\n",
        "import statistics as stats\r\n",
        "import math"
      ],
      "id": "WozBM1xn0cb1",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRl3HEsByA7P"
      },
      "source": [
        "def predict_score(model_init, X_train, X_test, y_train, y_test):\r\n",
        "    model = model_init\r\n",
        "    model = model.fit(X_train, y_train)\r\n",
        "    results = model.predict(X_test)\r\n",
        "    score = model.score(X_test, y_test) \r\n",
        "    return score, model.coef_\r\n"
      ],
      "id": "TRl3HEsByA7P",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQyPS1DuyA-o"
      },
      "source": [
        "def cross_val(data, model_init=None,sample_size=1000, train_fraction=0.7, features_size=60, h=1, date_start=None, date_end=None):\r\n",
        "    \r\n",
        "    data = select_date(data, date_start, date_end)\r\n",
        "    data_shifted = preprocessing_data(data, features_size, h)\r\n",
        "    X, y, data_size = features_target(data_shifted, h)\r\n",
        "    train_size = int(train_fraction*sample_size)\r\n",
        "    test_size = sample_size - train_size\r\n",
        "    \r\n",
        "    \r\n",
        "    r = math.floor((data_size-sample_size)/test_size)\r\n",
        "    intervals = range(0, r)\r\n",
        "    reversed_intervals = reversed(intervals)\r\n",
        "    results = []\r\n",
        "    parameters = []\r\n",
        "    \r\n",
        "    for i in reversed_intervals:\r\n",
        "        X_train, X_test, y_train, y_test = input_data(X, y, sample_size, data_size, train_size, test_size, h, w=i)\r\n",
        "        score, params = predict_score(model_init, X_train, X_test, y_train, y_test)\r\n",
        "        results.append(score)\r\n",
        "        parameters.append(params)\r\n",
        "        # print(f\"fold {i} done\")\r\n",
        "\r\n",
        "    \r\n",
        "    return dict({'mean_score':np.around(np.mean(results), 2),\r\n",
        "                 'std':np.around(np.std(results), 2),   \r\n",
        "                 'score_min':np.around(np.amin(results), 2),\r\n",
        "                 'score_max':np.around(np.amax(results), 2), \r\n",
        "                 'n_fold':r}), np.around(np.mean(parameters, axis=0), 4)"
      ],
      "id": "xQyPS1DuyA-o",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6maED2ro1tFV",
        "outputId": "ab0193bf-1688-4d09-fa86-a65e0c88b6d6"
      },
      "source": [
        "!pip install memoized_property"
      ],
      "id": "6maED2ro1tFV",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting memoized_property\n",
            "  Downloading https://files.pythonhosted.org/packages/70/db/23f8b5d86c9385299586c2469b58087f658f58eaeb414be0fd64cfd054e1/memoized-property-1.0.3.tar.gz\n",
            "Building wheels for collected packages: memoized-property\n",
            "  Building wheel for memoized-property (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for memoized-property: filename=memoized_property-1.0.3-py2.py3-none-any.whl size=4175 sha256=199aa0b3a9ed5dce0c7df34c040d8c91ab5074d5d00a8ec5a4c36bc4c6a8152b\n",
            "  Stored in directory: /root/.cache/pip/wheels/8e/d4/82/ace27b33257f5918497d20427c0ac31d39c76e68b44d593fe6\n",
            "Successfully built memoized-property\n",
            "Installing collected packages: memoized-property\n",
            "Successfully installed memoized-property-1.0.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGITxQ9911tO",
        "outputId": "b4e21766-75de-47cc-8002-1d24e17afee1"
      },
      "source": [
        "!pip install mlflow"
      ],
      "id": "EGITxQ9911tO",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mlflow\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/58/dc/b45061f1cde42465f8ac1ebd86db3253a0e155619929bf1d6de271317c08/mlflow-1.14.1-py3-none-any.whl (14.2MB)\n",
            "\u001b[K     |████████████████████████████████| 14.2MB 319kB/s \n",
            "\u001b[?25hCollecting docker>=4.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c4/22/410313ad554477e87ec406d38d85f810e61ddb0d2fc44e64994857476de9/docker-4.4.4-py2.py3-none-any.whl (147kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 48.1MB/s \n",
            "\u001b[?25hCollecting prometheus-flask-exporter\n",
            "  Downloading https://files.pythonhosted.org/packages/4c/d5/8a046d683c2cc084b6a502812827ede69b1064f95d93f94b83f809b21723/prometheus_flask_exporter-0.18.1.tar.gz\n",
            "Collecting gitpython>=2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/99/98019716955ba243657daedd1de8f3a88ca1f5b75057c38e959db22fb87b/GitPython-3.1.14-py3-none-any.whl (159kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 40.6MB/s \n",
            "\u001b[?25hCollecting gunicorn; platform_system != \"Windows\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/ca/926f7cd3a2014b16870086b2d0fdc84a9e49473c68a8dff8b57f7c156f43/gunicorn-20.0.4-py2.py3-none-any.whl (77kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 6.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from mlflow) (1.3.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from mlflow) (1.1.5)\n",
            "Requirement already satisfied: sqlparse>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from mlflow) (0.4.1)\n",
            "Requirement already satisfied: Flask in /usr/local/lib/python3.7/dist-packages (from mlflow) (1.1.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from mlflow) (3.13)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from mlflow) (2018.9)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mlflow) (1.19.5)\n",
            "Collecting querystring-parser\n",
            "  Downloading https://files.pythonhosted.org/packages/88/6b/572b2590fd55114118bf08bde63c0a421dcc82d593700f3e2ad89908a8a9/querystring_parser-1.2.4-py2.py3-none-any.whl\n",
            "Collecting databricks-cli>=0.8.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5b/44/69bb20181e59c4ccca22bf8444342994270e2e87111ab07f2b8d19a8c1b2/databricks-cli-0.14.2.tar.gz (54kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 5.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from mlflow) (7.1.2)\n",
            "Requirement already satisfied: requests>=2.17.3 in /usr/local/lib/python3.7/dist-packages (from mlflow) (2.23.0)\n",
            "Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.7/dist-packages (from mlflow) (1.3.23)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from mlflow) (0.3)\n",
            "Collecting alembic<=1.4.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e0/e9/359dbb77c35c419df0aedeb1d53e71e7e3f438ff64a8fdb048c907404de3/alembic-1.4.1.tar.gz (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 43.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from mlflow) (3.12.4)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from docker>=4.0.0->mlflow) (1.15.0)\n",
            "Collecting websocket-client>=0.32.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/33/80e0d4f60e84a1ddd9a03f340be1065a2a363c47ce65c4bd3bae65ce9631/websocket_client-0.58.0-py2.py3-none-any.whl (61kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 5.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: prometheus_client in /usr/local/lib/python3.7/dist-packages (from prometheus-flask-exporter->mlflow) (0.9.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/11/d1800bca0a3bae820b84b7d813ad1eff15a48a64caea9c823fc8c1b119e8/gitdb-4.0.5-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 5.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools>=3.0 in /usr/local/lib/python3.7/dist-packages (from gunicorn; platform_system != \"Windows\"->mlflow) (54.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->mlflow) (2.8.1)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask->mlflow) (1.1.0)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask->mlflow) (2.11.3)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask->mlflow) (1.0.1)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.7/dist-packages (from databricks-cli>=0.8.7->mlflow) (0.8.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.17.3->mlflow) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.17.3->mlflow) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.17.3->mlflow) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.17.3->mlflow) (3.0.4)\n",
            "Collecting Mako\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/db/2d2d88b924aa4674a080aae83b59ea19d593250bfe5ed789947c21736785/Mako-1.1.4.tar.gz (479kB)\n",
            "\u001b[K     |████████████████████████████████| 481kB 48.4MB/s \n",
            "\u001b[?25hCollecting python-editor>=0.3\n",
            "  Downloading https://files.pythonhosted.org/packages/c6/d3/201fc3abe391bbae6606e6f1d598c15d367033332bd54352b12f35513717/python_editor-1.0.4-py3-none-any.whl\n",
            "Collecting smmap<4,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/d5/1e/6130925131f639b2acde0f7f18b73e33ce082ff2d90783c436b52040af5a/smmap-3.0.5-py2.py3-none-any.whl\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.10.1->Flask->mlflow) (1.1.1)\n",
            "Building wheels for collected packages: prometheus-flask-exporter, databricks-cli, alembic, Mako\n",
            "  Building wheel for prometheus-flask-exporter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for prometheus-flask-exporter: filename=prometheus_flask_exporter-0.18.1-cp37-none-any.whl size=17159 sha256=ed63896471a2510cb098d415f41415adef820a4ec3212a4f85c899eab74ee05d\n",
            "  Stored in directory: /root/.cache/pip/wheels/b4/1f/b8/66bd9bc3a9d6c6987ff6c4dfeb6f1fe97b5a0e5ed5849c0437\n",
            "  Building wheel for databricks-cli (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for databricks-cli: filename=databricks_cli-0.14.2-cp37-none-any.whl size=100732 sha256=c3bf1b78686288e562df4f4e4f4e7717dca0a9067d10cf772476271228ced124\n",
            "  Stored in directory: /root/.cache/pip/wheels/b1/66/0a/72e45560f269c38892b8f42169a22403ff46e3811f2c82c3a6\n",
            "  Building wheel for alembic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for alembic: filename=alembic-1.4.1-py2.py3-none-any.whl size=158155 sha256=1c9d3286ff2f6558a7fbfe3b67ff4383b45bc80b92e0d8d5f237adac46bbd8e3\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/07/f7/12f7370ca47a66030c2edeedcc23dec26ea0ac22dcb4c4a0f3\n",
            "  Building wheel for Mako (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Mako: filename=Mako-1.1.4-py2.py3-none-any.whl size=75675 sha256=13b2276874b7df33e004ddfe5b5f10d321aada784380a612120a3a2c461b4d6d\n",
            "  Stored in directory: /root/.cache/pip/wheels/ad/10/d3/aeb26e20d19045e2a68e5d3cbb57432e11b5d9c92c99f98d47\n",
            "Successfully built prometheus-flask-exporter databricks-cli alembic Mako\n",
            "Installing collected packages: websocket-client, docker, prometheus-flask-exporter, smmap, gitdb, gitpython, gunicorn, querystring-parser, databricks-cli, Mako, python-editor, alembic, mlflow\n",
            "Successfully installed Mako-1.1.4 alembic-1.4.1 databricks-cli-0.14.2 docker-4.4.4 gitdb-4.0.5 gitpython-3.1.14 gunicorn-20.0.4 mlflow-1.14.1 prometheus-flask-exporter-0.18.1 python-editor-1.0.4 querystring-parser-1.2.4 smmap-3.0.5 websocket-client-0.58.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcjgRHvtyBBr"
      },
      "source": [
        "from memoized_property import memoized_property\r\n",
        "\r\n",
        "import mlflow\r\n",
        "from mlflow.tracking import MlflowClient\r\n",
        "\r\n",
        "\r\n",
        "class MLFlowBase():\r\n",
        "\r\n",
        "    def __init__(self, experiment_name, MLFLOW_URI):\r\n",
        "        self.experiment_name = experiment_name\r\n",
        "        self.MLFLOW_URI = MLFLOW_URI\r\n",
        "\r\n",
        "    @memoized_property\r\n",
        "    def mlflow_client(self):\r\n",
        "        mlflow.set_tracking_uri(self.MLFLOW_URI)\r\n",
        "        return MlflowClient()\r\n",
        "\r\n",
        "    @memoized_property\r\n",
        "    def mlflow_experiment_id(self):\r\n",
        "        try:\r\n",
        "            return self.mlflow_client \\\r\n",
        "                .create_experiment(self.experiment_name)\r\n",
        "        except BaseException:\r\n",
        "            return self.mlflow_client \\\r\n",
        "                .get_experiment_by_name(self.experiment_name).experiment_id\r\n",
        "\r\n",
        "    def mlflow_create_run(self):\r\n",
        "        self.mlflow_run = self.mlflow_client \\\r\n",
        "            .create_run(self.mlflow_experiment_id)\r\n",
        "\r\n",
        "    def mlflow_log_param(self, key, value):\r\n",
        "        self.mlflow_client \\\r\n",
        "            .log_param(self.mlflow_run.info.run_id, key, value)\r\n",
        "\r\n",
        "    def mlflow_log_metric(self, key, value):\r\n",
        "        self.mlflow_client \\\r\n",
        "            .log_metric(self.mlflow_run.info.run_id, key, value)"
      ],
      "id": "xcjgRHvtyBBr",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2P81nWE2yBEm"
      },
      "source": [
        "from itertools import product"
      ],
      "id": "2P81nWE2yBEm",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrXkKNRByBHX"
      },
      "source": [
        "class Trainer(MLFlowBase):\r\n",
        "\r\n",
        "    def __init__(self):\r\n",
        "        super().__init__(\r\n",
        "            \"[FR] [Paris] [nhuberde] bitcoin project\",\r\n",
        "            \"https://mlflow.lewagon.co\")\r\n",
        "            \r\n",
        "    # def train(self, trainer_params, hyper_params):\r\n",
        "    def train(self, trainer_params, data):\r\n",
        "\r\n",
        "        i = 0\r\n",
        "\r\n",
        "        # step 1 : iterate on trainer params\r\n",
        "        for param_combination in product(*trainer_params.values()):\r\n",
        "\r\n",
        "            exp_params = dict(zip(trainer_params.keys(), param_combination))\r\n",
        "\r\n",
        "            # print(exp_params)\r\n",
        "\r\n",
        "            # step 2 : iterate on models\r\n",
        "            # for model_name, model_hparams in hyper_params.items():\r\n",
        "\r\n",
        "                # print(f\"model name {model_name}\")\r\n",
        "\r\n",
        "                # step 3 : iterate on model hyperparams\r\n",
        "                # for hparam_combi in product(*model_hparams.values()):\r\n",
        "\r\n",
        "                #     hexp_params = dict(zip(model_hparams.keys(), hparam_combi))\r\n",
        "\r\n",
        "                    # print(hexp_params)\r\n",
        "\r\n",
        "                    # mais avec quoi je train ?\r\n",
        "            i += 1\r\n",
        "            print(f\"\\nexperiment #{i}:\")\r\n",
        "            print(exp_params)\r\n",
        "            # print(f\"model name {model_name}\")\r\n",
        "            # print(hexp_params)\r\n",
        "\r\n",
        "            # TODO: train with trainer params + model + hyperparams\r\n",
        "\r\n",
        "            # => appeler la crossval\r\n",
        "            # data = get_data()\r\n",
        "            results, parameters = cross_val(data, **exp_params)\r\n",
        "\r\n",
        "\r\n",
        "            # create a mlflow training\r\n",
        "            self.mlflow_create_run()  # create one training\r\n",
        "\r\n",
        "            # log trainer params\r\n",
        "            for key, value in exp_params.items():\r\n",
        "                self.mlflow_log_param(key, value)\r\n",
        "                \r\n",
        "            # then log buddy_name on mlflow\r\n",
        "            self.mlflow_log_param(\"buddy_name\", {buddy_name})\r\n",
        "\r\n",
        "            # log params\r\n",
        "            # self.mlflow_log_param(\"model\", model_name)\r\n",
        "\r\n",
        "            # log model hyper params\r\n",
        "            # for key, value in hexp_params.items():\r\n",
        "            #     self.mlflow_log_param(key, value)\r\n",
        "\r\n",
        "            # push metrics to mlflow\r\n",
        "            self.mlflow_log_metric(\"mean_score\", results['mean_score'])\r\n",
        "            self.mlflow_log_metric(\"std\", results['std'])\r\n",
        "            self.mlflow_log_metric(\"score_min\", results['score_min'])\r\n",
        "            self.mlflow_log_metric(\"score_max\", results['score_max'])\r\n",
        "            for p in range(parameters.shape[1]):\r\n",
        "                self.mlflow_log_metric(f\"coef_feature {p}\", parameters[0,p])\r\n",
        "            \r\n",
        "\r\n",
        "\r\n",
        "# if __name__ == \"__main__\":\r\n",
        "\r\n"
      ],
      "id": "IrXkKNRByBHX",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cw_4mJrG3rlE"
      },
      "source": [
        "buddy_name = 'Nicolas'"
      ],
      "id": "cw_4mJrG3rlE",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zspARyg4Ly-"
      },
      "source": [
        "from sklearn.svm import SVC\r\n",
        "from sklearn.linear_model import RidgeClassifier"
      ],
      "id": "4zspARyg4Ly-",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wim0FQ6yBKJ",
        "outputId": "e5c2413e-00f4-423f-8a5c-e5993983ab8e"
      },
      "source": [
        "trainer_params = dict(\r\n",
        "    model_init=[RidgeClassifier()],\r\n",
        "    sample_size=[1440, 10080, 14400, 21600, 43200, 86400, 129600],\r\n",
        "    train_fraction=[0.7],\r\n",
        "    features_size=[60],\r\n",
        "    h=[10],\r\n",
        "    date_start=[\"2020\"],\r\n",
        "    date_end=[\"2020\"],\r\n",
        ")\r\n",
        "\r\n",
        "trainer = Trainer()\r\n",
        "models = trainer.train(trainer_params, data)\r\n",
        "models"
      ],
      "id": "4wim0FQ6yBKJ",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "experiment #1:\n",
            "{'model_init': RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,\n",
            "                max_iter=None, normalize=False, random_state=None,\n",
            "                solver='auto', tol=0.001), 'sample_size': 1440, 'train_fraction': 0.7, 'features_size': 60, 'h': 10, 'date_start': '2020', 'date_end': '2020'}\n",
            "\n",
            "experiment #2:\n",
            "{'model_init': RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,\n",
            "                max_iter=None, normalize=False, random_state=None,\n",
            "                solver='auto', tol=0.001), 'sample_size': 10080, 'train_fraction': 0.7, 'features_size': 60, 'h': 10, 'date_start': '2020', 'date_end': '2020'}\n",
            "\n",
            "experiment #3:\n",
            "{'model_init': RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,\n",
            "                max_iter=None, normalize=False, random_state=None,\n",
            "                solver='auto', tol=0.001), 'sample_size': 14400, 'train_fraction': 0.7, 'features_size': 60, 'h': 10, 'date_start': '2020', 'date_end': '2020'}\n",
            "\n",
            "experiment #4:\n",
            "{'model_init': RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,\n",
            "                max_iter=None, normalize=False, random_state=None,\n",
            "                solver='auto', tol=0.001), 'sample_size': 21600, 'train_fraction': 0.7, 'features_size': 60, 'h': 10, 'date_start': '2020', 'date_end': '2020'}\n",
            "\n",
            "experiment #5:\n",
            "{'model_init': RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,\n",
            "                max_iter=None, normalize=False, random_state=None,\n",
            "                solver='auto', tol=0.001), 'sample_size': 43200, 'train_fraction': 0.7, 'features_size': 60, 'h': 10, 'date_start': '2020', 'date_end': '2020'}\n",
            "\n",
            "experiment #6:\n",
            "{'model_init': RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,\n",
            "                max_iter=None, normalize=False, random_state=None,\n",
            "                solver='auto', tol=0.001), 'sample_size': 86400, 'train_fraction': 0.7, 'features_size': 60, 'h': 10, 'date_start': '2020', 'date_end': '2020'}\n",
            "\n",
            "experiment #7:\n",
            "{'model_init': RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,\n",
            "                max_iter=None, normalize=False, random_state=None,\n",
            "                solver='auto', tol=0.001), 'sample_size': 129600, 'train_fraction': 0.7, 'features_size': 60, 'h': 10, 'date_start': '2020', 'date_end': '2020'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34Cj2EAcyBat"
      },
      "source": [
        ""
      ],
      "id": "34Cj2EAcyBat",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNJSoSwQ0KSJ"
      },
      "source": [
        "def predict_score_deep(X_train, X_test, y_train, y_test):\r\n",
        "    \r\n",
        "    model = Sequential()\r\n",
        "    es = EarlyStopping(patience=2, restore_best_weights=True)\r\n",
        "    \r\n",
        "    model.add(GRU(units = 30, activation='tanh', return_sequences = True, input_shape = (X_train.shape[1], 1)))\r\n",
        "    model.add(Dropout(0.2))\r\n",
        "    # model.add(LSTM(units = 40, return_sequences = True))\r\n",
        "    # model.add(Dropout(0.2))\r\n",
        "    model.add(GRU(units = 10,  activation='tanh', return_sequences = True))\r\n",
        "    model.add(Dropout(0.2))\r\n",
        "    # Adding the output layer\r\n",
        "    model.add(Dense(units = 5, activation='relu'))\r\n",
        "    model.add(Dense(units = 1, activation='sigmoid'))\r\n",
        "    # Compiling the RNN\r\n",
        "    model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\r\n",
        "    # Fitting the RNN to the Training set\r\n",
        "    model.fit(X_train, y_train, validation_split=0.2, epochs = 3, batch_size = 32, callbacks=[es])\r\n",
        "\r\n",
        "    score = model.evaluate(X_test, y_test, verbose=0)\r\n",
        "\r\n",
        "    return score[1] #attention score[0] loss à return also"
      ],
      "id": "nNJSoSwQ0KSJ",
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajIypFa70LFF"
      },
      "source": [
        "def deep(data, sample_size=1000, train_fraction=0.7, features_size=60, h=1, date_start=None, date_end=None):\r\n",
        "    \r\n",
        "    data = select_date(data, date_start, date_end)\r\n",
        "    data_shifted = preprocessing_data(data, features_size, h)\r\n",
        "    X, y, data_size = features_target(data_shifted, h)\r\n",
        "    train_size = int(train_fraction*sample_size)\r\n",
        "    test_size = sample_size - train_size\r\n",
        "    \r\n",
        "    r = math.floor((data_size-train_size)/test_size)\r\n",
        "    intervals = range(0, r)\r\n",
        "    reversed_intervals = reversed(intervals)\r\n",
        "    results = []\r\n",
        "    \r\n",
        "    for i in reversed_intervals:\r\n",
        "        X_train, X_test, y_train, y_test = input_data(X, y, sample_size, data_size, train_size, test_size, h, w=i)\r\n",
        "  \r\n",
        "        X_train, y_train = np.array(X_train), np.array(y_train)\r\n",
        "        X_test, y_test = np.array(X_test), np.array(y_test)\r\n",
        "        X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\r\n",
        "        X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\r\n",
        "            \r\n",
        "        score = predict_score_deep(X_train, X_test, y_train, y_test)\r\n",
        "        results.append(score)\r\n",
        "    \r\n",
        "    return dict({'mean_score':round(stats.mean(results),2),\r\n",
        "                 'std':round(stats.stdev(results),2),\r\n",
        "                 'score_min':round(min(results),2),\r\n",
        "                 'score_max':round(max(results),2),\r\n",
        "                 'n_fold':r})"
      ],
      "id": "ajIypFa70LFF",
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4qTBvzB2Yhi"
      },
      "source": [
        "class Trainer2(MLFlowBase):\r\n",
        "\r\n",
        "    def __init__(self):\r\n",
        "        super().__init__(\r\n",
        "            \"[FR] [Paris] [nhuberde] bitcoin project\",\r\n",
        "            \"https://mlflow.lewagon.co\")\r\n",
        "            \r\n",
        "    # def train(self, trainer_params, hyper_params):\r\n",
        "    def train(self, trainer_params, data):\r\n",
        "\r\n",
        "        i = 0\r\n",
        "\r\n",
        "        # step 1 : iterate on trainer params\r\n",
        "        for param_combination in product(*trainer_params.values()):\r\n",
        "\r\n",
        "            exp_params = dict(zip(trainer_params.keys(), param_combination))\r\n",
        "\r\n",
        "            # print(exp_params)\r\n",
        "\r\n",
        "            # step 2 : iterate on models\r\n",
        "            # for model_name, model_hparams in hyper_params.items():\r\n",
        "\r\n",
        "                # print(f\"model name {model_name}\")\r\n",
        "\r\n",
        "                # step 3 : iterate on model hyperparams\r\n",
        "                # for hparam_combi in product(*model_hparams.values()):\r\n",
        "\r\n",
        "                #     hexp_params = dict(zip(model_hparams.keys(), hparam_combi))\r\n",
        "\r\n",
        "                    # print(hexp_params)\r\n",
        "\r\n",
        "                    # mais avec quoi je train ?\r\n",
        "            i += 1\r\n",
        "            print(f\"\\nexperiment #{i}:\")\r\n",
        "            print(exp_params)\r\n",
        "            # print(f\"model name {model_name}\")\r\n",
        "            # print(hexp_params)\r\n",
        "\r\n",
        "            # TODO: train with trainer params + model + hyperparams\r\n",
        "\r\n",
        "            # => appeler la crossval\r\n",
        "            # data = get_data()\r\n",
        "            results = deep(data, **exp_params)\r\n",
        "\r\n",
        "\r\n",
        "            # create a mlflow training\r\n",
        "            self.mlflow_create_run()  # create one training\r\n",
        "\r\n",
        "            # log trainer params\r\n",
        "            for key, value in exp_params.items():\r\n",
        "                self.mlflow_log_param(key, value)\r\n",
        "                \r\n",
        "            # then log buddy_name on mlflow\r\n",
        "            self.mlflow_log_param(\"buddy_name\", {buddy_name})\r\n",
        "            self.mlflow_log_param(\"model_type\", \"GRU\")\r\n",
        "\r\n",
        "            # log params\r\n",
        "            # self.mlflow_log_param(\"model\", model_name)\r\n",
        "\r\n",
        "            # log model hyper params\r\n",
        "            # for key, value in hexp_params.items():\r\n",
        "            #     self.mlflow_log_param(key, value)\r\n",
        "\r\n",
        "            # push metrics to mlflow\r\n",
        "            self.mlflow_log_metric(\"mean_score\", results['mean_score'])\r\n",
        "            self.mlflow_log_metric(\"std\", results['std'])\r\n",
        "            self.mlflow_log_metric(\"score_min\", results['score_min'])\r\n",
        "            self.mlflow_log_metric(\"score_max\", results['score_max'])\r\n",
        "            self.mlflow_log_metric(\"n_fold\", results['n_fold'])\r\n",
        "            \r\n",
        "\r\n",
        "\r\n",
        "# if __name__ == \"__main__\":\r\n",
        "\r\n"
      ],
      "id": "T4qTBvzB2Yhi",
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jl__fA0q4Cce"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\r\n",
        "from tensorflow.keras import layers\r\n",
        "from tensorflow.keras.layers import GRU, Dense\r\n",
        "from tensorflow.keras.callbacks import EarlyStopping\r\n",
        "import math\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import keras\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.layers import LSTM\r\n",
        "from keras.layers import Dropout\r\n",
        "from keras.layers import *\r\n",
        "from sklearn.preprocessing import MinMaxScaler\r\n",
        "from sklearn.metrics import mean_squared_error\r\n",
        "from sklearn.metrics import mean_absolute_error\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from keras.callbacks import EarlyStopping"
      ],
      "id": "Jl__fA0q4Cce",
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eg9mNnhW0LI8"
      },
      "source": [
        "results = deep(data, sample_size=1440, train_fraction=0.7, features_size=60, h=1, date_start=\"2020-10-01\", date_end=\"2020-10-07\")"
      ],
      "id": "Eg9mNnhW0LI8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHsIZuXR0LMW",
        "outputId": "746e6f36-8f69-416f-969c-40e55af90e85"
      },
      "source": [
        "results"
      ],
      "id": "lHsIZuXR0LMW",
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'mean_score': 0.51,\n",
              " 'n_fold': 20,\n",
              " 'score_max': 0.61,\n",
              " 'score_min': 0.47,\n",
              " 'std': 0.03}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjSi59gH0LO6",
        "outputId": "bfb9606f-db05-4c17-9033-3190e619792a"
      },
      "source": [
        "trainer_params2 = dict(\r\n",
        "    sample_size=[1440],\r\n",
        "    train_fraction=[0.7],\r\n",
        "    features_size=[60],\r\n",
        "    h=[1],\r\n",
        "    date_start=[\"2020-10-01\"],\r\n",
        "    date_end=[\"2020-10-07\"],\r\n",
        ")\r\n",
        "\r\n",
        "trainer2 = Trainer2()\r\n",
        "trainer2.train(trainer_params2, data)"
      ],
      "id": "jjSi59gH0LO6",
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "experiment #1:\n",
            "{'sample_size': 1440, 'train_fraction': 0.7, 'features_size': 60, 'h': 1, 'date_start': '2020-10-01', 'date_end': '2020-10-07'}\n",
            "Epoch 1/3\n",
            "26/26 [==============================] - 6s 78ms/step - loss: 0.6929 - accuracy: 0.5298 - val_loss: 0.6968 - val_accuracy: 0.4752\n",
            "Epoch 2/3\n",
            "26/26 [==============================] - 1s 50ms/step - loss: 0.6888 - accuracy: 0.5559 - val_loss: 0.6966 - val_accuracy: 0.4752\n",
            "Epoch 3/3\n",
            "26/26 [==============================] - 1s 49ms/step - loss: 0.6893 - accuracy: 0.5562 - val_loss: 0.6973 - val_accuracy: 0.4752\n",
            "Epoch 1/3\n",
            "26/26 [==============================] - 6s 82ms/step - loss: 0.7268 - accuracy: 0.4835 - val_loss: 0.6942 - val_accuracy: 0.4802\n",
            "Epoch 2/3\n",
            "26/26 [==============================] - 1s 51ms/step - loss: 0.6965 - accuracy: 0.4991 - val_loss: 0.6936 - val_accuracy: 0.4802\n",
            "Epoch 3/3\n",
            "26/26 [==============================] - 1s 51ms/step - loss: 0.6944 - accuracy: 0.4971 - val_loss: 0.6937 - val_accuracy: 0.4802\n",
            "Epoch 1/3\n",
            "26/26 [==============================] - 6s 79ms/step - loss: 0.7005 - accuracy: 0.4973 - val_loss: 0.6921 - val_accuracy: 0.5297\n",
            "Epoch 2/3\n",
            "26/26 [==============================] - 1s 48ms/step - loss: 0.6933 - accuracy: 0.5071 - val_loss: 0.6932 - val_accuracy: 0.4713\n",
            "Epoch 3/3\n",
            "26/26 [==============================] - 1s 51ms/step - loss: 0.6940 - accuracy: 0.4945 - val_loss: 0.6931 - val_accuracy: 0.5248\n",
            "Epoch 1/3\n",
            "26/26 [==============================] - 6s 80ms/step - loss: 0.7514 - accuracy: 0.5059 - val_loss: 0.6996 - val_accuracy: 0.4802\n",
            "Epoch 2/3\n",
            "26/26 [==============================] - 1s 48ms/step - loss: 0.7019 - accuracy: 0.4963 - val_loss: 0.6989 - val_accuracy: 0.4802\n",
            "Epoch 3/3\n",
            "26/26 [==============================] - 1s 49ms/step - loss: 0.6997 - accuracy: 0.4855 - val_loss: 0.6947 - val_accuracy: 0.4802\n",
            "Epoch 1/3\n",
            "26/26 [==============================] - 6s 81ms/step - loss: 0.7296 - accuracy: 0.4828 - val_loss: 0.7115 - val_accuracy: 0.4356\n",
            "Epoch 2/3\n",
            "26/26 [==============================] - 1s 49ms/step - loss: 0.6998 - accuracy: 0.5055 - val_loss: 0.6970 - val_accuracy: 0.4356\n",
            "Epoch 3/3\n",
            "26/26 [==============================] - 1s 49ms/step - loss: 0.6964 - accuracy: 0.5015 - val_loss: 0.6991 - val_accuracy: 0.4356\n",
            "Epoch 1/3\n",
            "26/26 [==============================] - 6s 79ms/step - loss: 0.7000 - accuracy: 0.5219 - val_loss: 0.6924 - val_accuracy: 0.5248\n",
            "Epoch 2/3\n",
            "26/26 [==============================] - 1s 48ms/step - loss: 0.6933 - accuracy: 0.5236 - val_loss: 0.6926 - val_accuracy: 0.5239\n",
            "Epoch 3/3\n",
            "26/26 [==============================] - 1s 48ms/step - loss: 0.6942 - accuracy: 0.5061 - val_loss: 0.6929 - val_accuracy: 0.5198\n",
            "Epoch 1/3\n",
            "26/26 [==============================] - 6s 84ms/step - loss: 0.7041 - accuracy: 0.4989 - val_loss: 0.6991 - val_accuracy: 0.4554\n",
            "Epoch 2/3\n",
            "26/26 [==============================] - 1s 47ms/step - loss: 0.6960 - accuracy: 0.5041 - val_loss: 0.6973 - val_accuracy: 0.4554\n",
            "Epoch 3/3\n",
            "26/26 [==============================] - 1s 45ms/step - loss: 0.6931 - accuracy: 0.5181 - val_loss: 0.6919 - val_accuracy: 0.5446\n",
            "Epoch 1/3\n",
            "26/26 [==============================] - 6s 81ms/step - loss: 0.6935 - accuracy: 0.5253 - val_loss: 0.6932 - val_accuracy: 0.4901\n",
            "Epoch 2/3\n",
            "26/26 [==============================] - 1s 49ms/step - loss: 0.6935 - accuracy: 0.5062 - val_loss: 0.6932 - val_accuracy: 0.4901\n",
            "Epoch 3/3\n",
            "26/26 [==============================] - 1s 42ms/step - loss: 0.6933 - accuracy: 0.4739 - val_loss: 0.6932 - val_accuracy: 0.4901\n",
            "Epoch 1/3\n",
            "26/26 [==============================] - 6s 87ms/step - loss: 0.6968 - accuracy: 0.4980 - val_loss: 0.6933 - val_accuracy: 0.4356\n",
            "Epoch 2/3\n",
            "26/26 [==============================] - 1s 47ms/step - loss: 0.6942 - accuracy: 0.4891 - val_loss: 0.6934 - val_accuracy: 0.4356\n",
            "Epoch 3/3\n",
            "26/26 [==============================] - 1s 47ms/step - loss: 0.6935 - accuracy: 0.5108 - val_loss: 0.6934 - val_accuracy: 0.4356\n",
            "Epoch 1/3\n",
            "26/26 [==============================] - 6s 83ms/step - loss: 0.6959 - accuracy: 0.5169 - val_loss: 0.6903 - val_accuracy: 0.5446\n",
            "Epoch 2/3\n",
            "26/26 [==============================] - 1s 49ms/step - loss: 0.6903 - accuracy: 0.5500 - val_loss: 0.6907 - val_accuracy: 0.5446\n",
            "Epoch 3/3\n",
            "26/26 [==============================] - 1s 50ms/step - loss: 0.6929 - accuracy: 0.5303 - val_loss: 0.6909 - val_accuracy: 0.5446\n",
            "Epoch 1/3\n",
            "26/26 [==============================] - 6s 82ms/step - loss: 0.7047 - accuracy: 0.4983 - val_loss: 0.6915 - val_accuracy: 0.5297\n",
            "Epoch 2/3\n",
            "26/26 [==============================] - 1s 48ms/step - loss: 0.6903 - accuracy: 0.5476 - val_loss: 0.6933 - val_accuracy: 0.4812\n",
            "Epoch 3/3\n",
            "26/26 [==============================] - 1s 47ms/step - loss: 0.6938 - accuracy: 0.5179 - val_loss: 0.6931 - val_accuracy: 0.5297\n",
            "Epoch 1/3\n",
            "26/26 [==============================] - 6s 94ms/step - loss: 0.7463 - accuracy: 0.4657 - val_loss: 0.6841 - val_accuracy: 0.5670\n",
            "Epoch 2/3\n",
            "26/26 [==============================] - 1s 47ms/step - loss: 0.7046 - accuracy: 0.5046 - val_loss: 0.6936 - val_accuracy: 0.5416\n",
            "Epoch 3/3\n",
            "26/26 [==============================] - 1s 47ms/step - loss: 0.7042 - accuracy: 0.4950 - val_loss: 0.6848 - val_accuracy: 0.5693\n",
            "Epoch 1/3\n",
            "26/26 [==============================] - 6s 75ms/step - loss: 0.6970 - accuracy: 0.5052 - val_loss: 0.7006 - val_accuracy: 0.4802\n",
            "Epoch 2/3\n",
            "26/26 [==============================] - 1s 48ms/step - loss: 0.6922 - accuracy: 0.5375 - val_loss: 0.6980 - val_accuracy: 0.4802\n",
            "Epoch 3/3\n",
            "26/26 [==============================] - 1s 48ms/step - loss: 0.6895 - accuracy: 0.5507 - val_loss: 0.6972 - val_accuracy: 0.4802\n",
            "Epoch 1/3\n",
            "26/26 [==============================] - 6s 84ms/step - loss: 0.7165 - accuracy: 0.4922 - val_loss: 0.7077 - val_accuracy: 0.4901\n",
            "Epoch 2/3\n",
            "26/26 [==============================] - 1s 50ms/step - loss: 0.7018 - accuracy: 0.5193 - val_loss: 0.6981 - val_accuracy: 0.4901\n",
            "Epoch 3/3\n",
            "26/26 [==============================] - 1s 49ms/step - loss: 0.6973 - accuracy: 0.5123 - val_loss: 0.6947 - val_accuracy: 0.4901\n",
            "Epoch 1/3\n",
            "26/26 [==============================] - 6s 84ms/step - loss: 0.7203 - accuracy: 0.4768 - val_loss: 0.6980 - val_accuracy: 0.4554\n",
            "Epoch 2/3\n",
            "26/26 [==============================] - 1s 51ms/step - loss: 0.6979 - accuracy: 0.4822 - val_loss: 0.6922 - val_accuracy: 0.5446\n",
            "Epoch 3/3\n",
            "26/26 [==============================] - 1s 50ms/step - loss: 0.6936 - accuracy: 0.5094 - val_loss: 0.6921 - val_accuracy: 0.5446\n",
            "Epoch 1/3\n",
            "26/26 [==============================] - 6s 87ms/step - loss: 0.7005 - accuracy: 0.5269 - val_loss: 0.6834 - val_accuracy: 0.5941\n",
            "Epoch 2/3\n",
            "26/26 [==============================] - 1s 49ms/step - loss: 0.7048 - accuracy: 0.5011 - val_loss: 0.6843 - val_accuracy: 0.5941\n",
            "Epoch 3/3\n",
            "26/26 [==============================] - 1s 48ms/step - loss: 0.6983 - accuracy: 0.5135 - val_loss: 0.6859 - val_accuracy: 0.5941\n",
            "Epoch 1/3\n",
            "26/26 [==============================] - 6s 82ms/step - loss: 0.6968 - accuracy: 0.5426 - val_loss: 0.6901 - val_accuracy: 0.5396\n",
            "Epoch 2/3\n",
            "26/26 [==============================] - 1s 51ms/step - loss: 0.6885 - accuracy: 0.5568 - val_loss: 0.6904 - val_accuracy: 0.5396\n",
            "Epoch 3/3\n",
            "26/26 [==============================] - 1s 53ms/step - loss: 0.6879 - accuracy: 0.5558 - val_loss: 0.6905 - val_accuracy: 0.5396\n",
            "Epoch 1/3\n",
            "26/26 [==============================] - 6s 85ms/step - loss: 0.6928 - accuracy: 0.5365 - val_loss: 0.6934 - val_accuracy: 0.5050\n",
            "Epoch 2/3\n",
            "26/26 [==============================] - 1s 50ms/step - loss: 0.6940 - accuracy: 0.5130 - val_loss: 0.6934 - val_accuracy: 0.5050\n",
            "Epoch 3/3\n",
            "26/26 [==============================] - 1s 50ms/step - loss: 0.6893 - accuracy: 0.5586 - val_loss: 0.6935 - val_accuracy: 0.5050\n",
            "Epoch 1/3\n",
            "26/26 [==============================] - 6s 78ms/step - loss: 0.7075 - accuracy: 0.5106 - val_loss: 0.6958 - val_accuracy: 0.5050\n",
            "Epoch 2/3\n",
            "26/26 [==============================] - 1s 48ms/step - loss: 0.7013 - accuracy: 0.5034 - val_loss: 0.6933 - val_accuracy: 0.4967\n",
            "Epoch 3/3\n",
            "26/26 [==============================] - 1s 47ms/step - loss: 0.6979 - accuracy: 0.5019 - val_loss: 0.6933 - val_accuracy: 0.4965\n",
            "Epoch 1/3\n",
            "26/26 [==============================] - 6s 81ms/step - loss: 0.6941 - accuracy: 0.4864 - val_loss: 0.6930 - val_accuracy: 0.5198\n",
            "Epoch 2/3\n",
            "26/26 [==============================] - 1s 50ms/step - loss: 0.6933 - accuracy: 0.4939 - val_loss: 0.6928 - val_accuracy: 0.5198\n",
            "Epoch 3/3\n",
            "26/26 [==============================] - 1s 47ms/step - loss: 0.6927 - accuracy: 0.5267 - val_loss: 0.6929 - val_accuracy: 0.5198\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFOnRz3T9gX9"
      },
      "source": [
        "# data2 = select_date(data, date_start=None, date_end=None)"
      ],
      "id": "PFOnRz3T9gX9",
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-P09qyOF9q8m"
      },
      "source": [
        "# data2"
      ],
      "id": "-P09qyOF9q8m",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cftLHVz_0LR2"
      },
      "source": [
        "# data_shifted = preprocessing_data(data2, features_size=120, h=10)"
      ],
      "id": "cftLHVz_0LR2",
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 786
        },
        "id": "DxjudxYe0LVY",
        "outputId": "7584f037-05a2-453b-f944-6b7b5367ba76"
      },
      "source": [
        "# data_shifted"
      ],
      "id": "DxjudxYe0LVY",
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Open</th>\n",
              "      <th>diff_Open</th>\n",
              "      <th>t+10</th>\n",
              "      <th>t-0</th>\n",
              "      <th>t-1</th>\n",
              "      <th>t-2</th>\n",
              "      <th>t-3</th>\n",
              "      <th>t-4</th>\n",
              "      <th>t-5</th>\n",
              "      <th>t-6</th>\n",
              "      <th>t-7</th>\n",
              "      <th>t-8</th>\n",
              "      <th>t-9</th>\n",
              "      <th>t-10</th>\n",
              "      <th>t-11</th>\n",
              "      <th>t-12</th>\n",
              "      <th>t-13</th>\n",
              "      <th>t-14</th>\n",
              "      <th>t-15</th>\n",
              "      <th>t-16</th>\n",
              "      <th>t-17</th>\n",
              "      <th>t-18</th>\n",
              "      <th>t-19</th>\n",
              "      <th>t-20</th>\n",
              "      <th>t-21</th>\n",
              "      <th>t-22</th>\n",
              "      <th>t-23</th>\n",
              "      <th>t-24</th>\n",
              "      <th>t-25</th>\n",
              "      <th>t-26</th>\n",
              "      <th>t-27</th>\n",
              "      <th>t-28</th>\n",
              "      <th>t-29</th>\n",
              "      <th>t-30</th>\n",
              "      <th>t-31</th>\n",
              "      <th>t-32</th>\n",
              "      <th>t-33</th>\n",
              "      <th>t-34</th>\n",
              "      <th>t-35</th>\n",
              "      <th>t-36</th>\n",
              "      <th>...</th>\n",
              "      <th>t-80</th>\n",
              "      <th>t-81</th>\n",
              "      <th>t-82</th>\n",
              "      <th>t-83</th>\n",
              "      <th>t-84</th>\n",
              "      <th>t-85</th>\n",
              "      <th>t-86</th>\n",
              "      <th>t-87</th>\n",
              "      <th>t-88</th>\n",
              "      <th>t-89</th>\n",
              "      <th>t-90</th>\n",
              "      <th>t-91</th>\n",
              "      <th>t-92</th>\n",
              "      <th>t-93</th>\n",
              "      <th>t-94</th>\n",
              "      <th>t-95</th>\n",
              "      <th>t-96</th>\n",
              "      <th>t-97</th>\n",
              "      <th>t-98</th>\n",
              "      <th>t-99</th>\n",
              "      <th>t-100</th>\n",
              "      <th>t-101</th>\n",
              "      <th>t-102</th>\n",
              "      <th>t-103</th>\n",
              "      <th>t-104</th>\n",
              "      <th>t-105</th>\n",
              "      <th>t-106</th>\n",
              "      <th>t-107</th>\n",
              "      <th>t-108</th>\n",
              "      <th>t-109</th>\n",
              "      <th>t-110</th>\n",
              "      <th>t-111</th>\n",
              "      <th>t-112</th>\n",
              "      <th>t-113</th>\n",
              "      <th>t-114</th>\n",
              "      <th>t-115</th>\n",
              "      <th>t-116</th>\n",
              "      <th>t-117</th>\n",
              "      <th>t-118</th>\n",
              "      <th>t-119</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Timestamp</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2017-05-01 01:59:00</th>\n",
              "      <td>1348.50</td>\n",
              "      <td>-1.55</td>\n",
              "      <td>2.91</td>\n",
              "      <td>1348.50</td>\n",
              "      <td>1347.00</td>\n",
              "      <td>1347.00</td>\n",
              "      <td>1350.00</td>\n",
              "      <td>1352.95</td>\n",
              "      <td>1352.95</td>\n",
              "      <td>1352.95</td>\n",
              "      <td>1352.95</td>\n",
              "      <td>1350.05</td>\n",
              "      <td>1350.05</td>\n",
              "      <td>1350.05</td>\n",
              "      <td>1350.05</td>\n",
              "      <td>1350.05</td>\n",
              "      <td>1350.05</td>\n",
              "      <td>1350.05</td>\n",
              "      <td>1347.50</td>\n",
              "      <td>1347.00</td>\n",
              "      <td>1347.00</td>\n",
              "      <td>1347.00</td>\n",
              "      <td>1347.00</td>\n",
              "      <td>1347.00</td>\n",
              "      <td>1347.00</td>\n",
              "      <td>1347.00</td>\n",
              "      <td>1342.74</td>\n",
              "      <td>1346.77</td>\n",
              "      <td>1342.40</td>\n",
              "      <td>1342.40</td>\n",
              "      <td>1348.00</td>\n",
              "      <td>1348.00</td>\n",
              "      <td>1348.00</td>\n",
              "      <td>1348.00</td>\n",
              "      <td>1347.99</td>\n",
              "      <td>1345.13</td>\n",
              "      <td>1344.30</td>\n",
              "      <td>1350.75</td>\n",
              "      <td>1351.81</td>\n",
              "      <td>1351.82</td>\n",
              "      <td>...</td>\n",
              "      <td>1356.00</td>\n",
              "      <td>1354.64</td>\n",
              "      <td>1354.63</td>\n",
              "      <td>1355.99</td>\n",
              "      <td>1354.18</td>\n",
              "      <td>1353.19</td>\n",
              "      <td>1352.70</td>\n",
              "      <td>1353.03</td>\n",
              "      <td>1352.57</td>\n",
              "      <td>1352.57</td>\n",
              "      <td>1353.95</td>\n",
              "      <td>1352.04</td>\n",
              "      <td>1353.28</td>\n",
              "      <td>1352.22</td>\n",
              "      <td>1354.00</td>\n",
              "      <td>1353.71</td>\n",
              "      <td>1353.24</td>\n",
              "      <td>1351.24</td>\n",
              "      <td>1354.09</td>\n",
              "      <td>1354.67</td>\n",
              "      <td>1354.53</td>\n",
              "      <td>1354.53</td>\n",
              "      <td>1354.09</td>\n",
              "      <td>1354.09</td>\n",
              "      <td>1354.42</td>\n",
              "      <td>1353.22</td>\n",
              "      <td>1354.43</td>\n",
              "      <td>1355.37</td>\n",
              "      <td>1350.73</td>\n",
              "      <td>1351.25</td>\n",
              "      <td>1351.24</td>\n",
              "      <td>1351.24</td>\n",
              "      <td>1351.24</td>\n",
              "      <td>1349.47</td>\n",
              "      <td>1351.24</td>\n",
              "      <td>1351.25</td>\n",
              "      <td>1350.11</td>\n",
              "      <td>1349.49</td>\n",
              "      <td>1352.41</td>\n",
              "      <td>1348.88</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-05-01 02:00:00</th>\n",
              "      <td>1348.51</td>\n",
              "      <td>-1.54</td>\n",
              "      <td>3.38</td>\n",
              "      <td>1348.51</td>\n",
              "      <td>1348.50</td>\n",
              "      <td>1347.00</td>\n",
              "      <td>1347.00</td>\n",
              "      <td>1350.00</td>\n",
              "      <td>1352.95</td>\n",
              "      <td>1352.95</td>\n",
              "      <td>1352.95</td>\n",
              "      <td>1352.95</td>\n",
              "      <td>1350.05</td>\n",
              "      <td>1350.05</td>\n",
              "      <td>1350.05</td>\n",
              "      <td>1350.05</td>\n",
              "      <td>1350.05</td>\n",
              "      <td>1350.05</td>\n",
              "      <td>1350.05</td>\n",
              "      <td>1347.50</td>\n",
              "      <td>1347.00</td>\n",
              "      <td>1347.00</td>\n",
              "      <td>1347.00</td>\n",
              "      <td>1347.00</td>\n",
              "      <td>1347.00</td>\n",
              "      <td>1347.00</td>\n",
              "      <td>1347.00</td>\n",
              "      <td>1342.74</td>\n",
              "      <td>1346.77</td>\n",
              "      <td>1342.40</td>\n",
              "      <td>1342.40</td>\n",
              "      <td>1348.00</td>\n",
              "      <td>1348.00</td>\n",
              "      <td>1348.00</td>\n",
              "      <td>1348.00</td>\n",
              "      <td>1347.99</td>\n",
              "      <td>1345.13</td>\n",
              "      <td>1344.30</td>\n",
              "      <td>1350.75</td>\n",
              "      <td>1351.81</td>\n",
              "      <td>...</td>\n",
              "      <td>1355.59</td>\n",
              "      <td>1356.00</td>\n",
              "      <td>1354.64</td>\n",
              "      <td>1354.63</td>\n",
              "      <td>1355.99</td>\n",
              "      <td>1354.18</td>\n",
              "      <td>1353.19</td>\n",
              "      <td>1352.70</td>\n",
              "      <td>1353.03</td>\n",
              "      <td>1352.57</td>\n",
              "      <td>1352.57</td>\n",
              "      <td>1353.95</td>\n",
              "      <td>1352.04</td>\n",
              "      <td>1353.28</td>\n",
              "      <td>1352.22</td>\n",
              "      <td>1354.00</td>\n",
              "      <td>1353.71</td>\n",
              "      <td>1353.24</td>\n",
              "      <td>1351.24</td>\n",
              "      <td>1354.09</td>\n",
              "      <td>1354.67</td>\n",
              "      <td>1354.53</td>\n",
              "      <td>1354.53</td>\n",
              "      <td>1354.09</td>\n",
              "      <td>1354.09</td>\n",
              "      <td>1354.42</td>\n",
              "      <td>1353.22</td>\n",
              "      <td>1354.43</td>\n",
              "      <td>1355.37</td>\n",
              "      <td>1350.73</td>\n",
              "      <td>1351.25</td>\n",
              "      <td>1351.24</td>\n",
              "      <td>1351.24</td>\n",
              "      <td>1351.24</td>\n",
              "      <td>1349.47</td>\n",
              "      <td>1351.24</td>\n",
              "      <td>1351.25</td>\n",
              "      <td>1350.11</td>\n",
              "      <td>1349.49</td>\n",
              "      <td>1352.41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-05-01 02:01:00</th>\n",
              "      <td>1345.51</td>\n",
              "      <td>-4.54</td>\n",
              "      <td>5.57</td>\n",
              "      <td>1345.51</td>\n",
              "      <td>1348.51</td>\n",
              "      <td>1348.50</td>\n",
              "      <td>1347.00</td>\n",
              "      <td>1347.00</td>\n",
              "      <td>1350.00</td>\n",
              "      <td>1352.95</td>\n",
              "      <td>1352.95</td>\n",
              "      <td>1352.95</td>\n",
              "      <td>1352.95</td>\n",
              "      <td>1350.05</td>\n",
              "      <td>1350.05</td>\n",
              "      <td>1350.05</td>\n",
              "      <td>1350.05</td>\n",
              "      <td>1350.05</td>\n",
              "      <td>1350.05</td>\n",
              "      <td>1350.05</td>\n",
              "      <td>1347.50</td>\n",
              "      <td>1347.00</td>\n",
              "      <td>1347.00</td>\n",
              "      <td>1347.00</td>\n",
              "      <td>1347.00</td>\n",
              "      <td>1347.00</td>\n",
              "      <td>1347.00</td>\n",
              "      <td>1347.00</td>\n",
              "      <td>1342.74</td>\n",
              "      <td>1346.77</td>\n",
              "      <td>1342.40</td>\n",
              "      <td>1342.40</td>\n",
              "      <td>1348.00</td>\n",
              "      <td>1348.00</td>\n",
              "      <td>1348.00</td>\n",
              "      <td>1348.00</td>\n",
              "      <td>1347.99</td>\n",
              "      <td>1345.13</td>\n",
              "      <td>1344.30</td>\n",
              "      <td>1350.75</td>\n",
              "      <td>...</td>\n",
              "      <td>1356.00</td>\n",
              "      <td>1355.59</td>\n",
              "      <td>1356.00</td>\n",
              "      <td>1354.64</td>\n",
              "      <td>1354.63</td>\n",
              "      <td>1355.99</td>\n",
              "      <td>1354.18</td>\n",
              "      <td>1353.19</td>\n",
              "      <td>1352.70</td>\n",
              "      <td>1353.03</td>\n",
              "      <td>1352.57</td>\n",
              "      <td>1352.57</td>\n",
              "      <td>1353.95</td>\n",
              "      <td>1352.04</td>\n",
              "      <td>1353.28</td>\n",
              "      <td>1352.22</td>\n",
              "      <td>1354.00</td>\n",
              "      <td>1353.71</td>\n",
              "      <td>1353.24</td>\n",
              "      <td>1351.24</td>\n",
              "      <td>1354.09</td>\n",
              "      <td>1354.67</td>\n",
              "      <td>1354.53</td>\n",
              "      <td>1354.53</td>\n",
              "      <td>1354.09</td>\n",
              "      <td>1354.09</td>\n",
              "      <td>1354.42</td>\n",
              "      <td>1353.22</td>\n",
              "      <td>1354.43</td>\n",
              "      <td>1355.37</td>\n",
              "      <td>1350.73</td>\n",
              "      <td>1351.25</td>\n",
              "      <td>1351.24</td>\n",
              "      <td>1351.24</td>\n",
              "      <td>1351.24</td>\n",
              "      <td>1349.47</td>\n",
              "      <td>1351.24</td>\n",
              "      <td>1351.25</td>\n",
              "      <td>1350.11</td>\n",
              "      <td>1349.49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-05-01 02:02:00</th>\n",
              "      <td>1346.06</td>\n",
              "      <td>-6.89</td>\n",
              "      <td>7.10</td>\n",
              "      <td>1346.06</td>\n",
              "      <td>1345.51</td>\n",
              "      <td>1348.51</td>\n",
              "      <td>1348.50</td>\n",
              "      <td>1347.00</td>\n",
              "      <td>1347.00</td>\n",
              "      <td>1350.00</td>\n",
              "      <td>1352.95</td>\n",
              "      <td>1352.95</td>\n",
              "      <td>1352.95</td>\n",
              "      <td>1352.95</td>\n",
              "      <td>1350.05</td>\n",
              "      <td>1350.05</td>\n",
              "      <td>1350.05</td>\n",
              "      <td>1350.05</td>\n",
              "      <td>1350.05</td>\n",
              "      <td>1350.05</td>\n",
              "      <td>1350.05</td>\n",
              "      <td>1347.50</td>\n",
              "      <td>1347.00</td>\n",
              "      <td>1347.00</td>\n",
              "      <td>1347.00</td>\n",
              "      <td>1347.00</td>\n",
              "      <td>1347.00</td>\n",
              "      <td>1347.00</td>\n",
              "      <td>1347.00</td>\n",
              "      <td>1342.74</td>\n",
              "      <td>1346.77</td>\n",
              "      <td>1342.40</td>\n",
              "      <td>1342.40</td>\n",
              "      <td>1348.00</td>\n",
              "      <td>1348.00</td>\n",
              "      <td>1348.00</td>\n",
              "      <td>1348.00</td>\n",
              "      <td>1347.99</td>\n",
              "      <td>1345.13</td>\n",
              "      <td>1344.30</td>\n",
              "      <td>...</td>\n",
              "      <td>1356.04</td>\n",
              "      <td>1356.00</td>\n",
              "      <td>1355.59</td>\n",
              "      <td>1356.00</td>\n",
              "      <td>1354.64</td>\n",
              "      <td>1354.63</td>\n",
              "      <td>1355.99</td>\n",
              "      <td>1354.18</td>\n",
              "      <td>1353.19</td>\n",
              "      <td>1352.70</td>\n",
              "      <td>1353.03</td>\n",
              "      <td>1352.57</td>\n",
              "      <td>1352.57</td>\n",
              "      <td>1353.95</td>\n",
              "      <td>1352.04</td>\n",
              "      <td>1353.28</td>\n",
              "      <td>1352.22</td>\n",
              "      <td>1354.00</td>\n",
              "      <td>1353.71</td>\n",
              "      <td>1353.24</td>\n",
              "      <td>1351.24</td>\n",
              "      <td>1354.09</td>\n",
              "      <td>1354.67</td>\n",
              "      <td>1354.53</td>\n",
              "      <td>1354.53</td>\n",
              "      <td>1354.09</td>\n",
              "      <td>1354.09</td>\n",
              "      <td>1354.42</td>\n",
              "      <td>1353.22</td>\n",
              "      <td>1354.43</td>\n",
              "      <td>1355.37</td>\n",
              "      <td>1350.73</td>\n",
              "      <td>1351.25</td>\n",
              "      <td>1351.24</td>\n",
              "      <td>1351.24</td>\n",
              "      <td>1351.24</td>\n",
              "      <td>1349.47</td>\n",
              "      <td>1351.24</td>\n",
              "      <td>1351.25</td>\n",
              "      <td>1350.11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-05-01 02:03:00</th>\n",
              "      <td>1346.06</td>\n",
              "      <td>-6.89</td>\n",
              "      <td>5.48</td>\n",
              "      <td>1346.06</td>\n",
              "      <td>1346.06</td>\n",
              "      <td>1345.51</td>\n",
              "      <td>1348.51</td>\n",
              "      <td>1348.50</td>\n",
              "      <td>1347.00</td>\n",
              "      <td>1347.00</td>\n",
              "      <td>1350.00</td>\n",
              "      <td>1352.95</td>\n",
              "      <td>1352.95</td>\n",
              "      <td>1352.95</td>\n",
              "      <td>1352.95</td>\n",
              "      <td>1350.05</td>\n",
              "      <td>1350.05</td>\n",
              "      <td>1350.05</td>\n",
              "      <td>1350.05</td>\n",
              "      <td>1350.05</td>\n",
              "      <td>1350.05</td>\n",
              "      <td>1350.05</td>\n",
              "      <td>1347.50</td>\n",
              "      <td>1347.00</td>\n",
              "      <td>1347.00</td>\n",
              "      <td>1347.00</td>\n",
              "      <td>1347.00</td>\n",
              "      <td>1347.00</td>\n",
              "      <td>1347.00</td>\n",
              "      <td>1347.00</td>\n",
              "      <td>1342.74</td>\n",
              "      <td>1346.77</td>\n",
              "      <td>1342.40</td>\n",
              "      <td>1342.40</td>\n",
              "      <td>1348.00</td>\n",
              "      <td>1348.00</td>\n",
              "      <td>1348.00</td>\n",
              "      <td>1348.00</td>\n",
              "      <td>1347.99</td>\n",
              "      <td>1345.13</td>\n",
              "      <td>...</td>\n",
              "      <td>1356.04</td>\n",
              "      <td>1356.04</td>\n",
              "      <td>1356.00</td>\n",
              "      <td>1355.59</td>\n",
              "      <td>1356.00</td>\n",
              "      <td>1354.64</td>\n",
              "      <td>1354.63</td>\n",
              "      <td>1355.99</td>\n",
              "      <td>1354.18</td>\n",
              "      <td>1353.19</td>\n",
              "      <td>1352.70</td>\n",
              "      <td>1353.03</td>\n",
              "      <td>1352.57</td>\n",
              "      <td>1352.57</td>\n",
              "      <td>1353.95</td>\n",
              "      <td>1352.04</td>\n",
              "      <td>1353.28</td>\n",
              "      <td>1352.22</td>\n",
              "      <td>1354.00</td>\n",
              "      <td>1353.71</td>\n",
              "      <td>1353.24</td>\n",
              "      <td>1351.24</td>\n",
              "      <td>1354.09</td>\n",
              "      <td>1354.67</td>\n",
              "      <td>1354.53</td>\n",
              "      <td>1354.53</td>\n",
              "      <td>1354.09</td>\n",
              "      <td>1354.09</td>\n",
              "      <td>1354.42</td>\n",
              "      <td>1353.22</td>\n",
              "      <td>1354.43</td>\n",
              "      <td>1355.37</td>\n",
              "      <td>1350.73</td>\n",
              "      <td>1351.25</td>\n",
              "      <td>1351.24</td>\n",
              "      <td>1351.24</td>\n",
              "      <td>1351.24</td>\n",
              "      <td>1349.47</td>\n",
              "      <td>1351.24</td>\n",
              "      <td>1351.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-12-30 23:45:00</th>\n",
              "      <td>28838.89</td>\n",
              "      <td>-12.31</td>\n",
              "      <td>-29.82</td>\n",
              "      <td>28838.89</td>\n",
              "      <td>28860.66</td>\n",
              "      <td>28882.50</td>\n",
              "      <td>28866.78</td>\n",
              "      <td>28868.04</td>\n",
              "      <td>28874.00</td>\n",
              "      <td>28890.75</td>\n",
              "      <td>28919.92</td>\n",
              "      <td>28917.27</td>\n",
              "      <td>28899.00</td>\n",
              "      <td>28851.20</td>\n",
              "      <td>28859.08</td>\n",
              "      <td>28846.23</td>\n",
              "      <td>28859.00</td>\n",
              "      <td>28869.79</td>\n",
              "      <td>28835.96</td>\n",
              "      <td>28850.00</td>\n",
              "      <td>28834.94</td>\n",
              "      <td>28839.21</td>\n",
              "      <td>28835.01</td>\n",
              "      <td>28835.01</td>\n",
              "      <td>28864.23</td>\n",
              "      <td>28820.35</td>\n",
              "      <td>28842.28</td>\n",
              "      <td>28809.16</td>\n",
              "      <td>28791.40</td>\n",
              "      <td>28766.83</td>\n",
              "      <td>28720.00</td>\n",
              "      <td>28712.09</td>\n",
              "      <td>28666.83</td>\n",
              "      <td>28686.71</td>\n",
              "      <td>28664.75</td>\n",
              "      <td>28711.00</td>\n",
              "      <td>28697.52</td>\n",
              "      <td>28732.34</td>\n",
              "      <td>28719.98</td>\n",
              "      <td>28771.06</td>\n",
              "      <td>...</td>\n",
              "      <td>28829.90</td>\n",
              "      <td>28765.81</td>\n",
              "      <td>28766.09</td>\n",
              "      <td>28740.21</td>\n",
              "      <td>28750.61</td>\n",
              "      <td>28784.50</td>\n",
              "      <td>28783.40</td>\n",
              "      <td>28752.90</td>\n",
              "      <td>28796.92</td>\n",
              "      <td>28804.12</td>\n",
              "      <td>28877.48</td>\n",
              "      <td>28824.44</td>\n",
              "      <td>28832.33</td>\n",
              "      <td>28794.29</td>\n",
              "      <td>28841.73</td>\n",
              "      <td>28860.21</td>\n",
              "      <td>28860.53</td>\n",
              "      <td>28835.55</td>\n",
              "      <td>28899.17</td>\n",
              "      <td>28889.85</td>\n",
              "      <td>28936.77</td>\n",
              "      <td>28925.78</td>\n",
              "      <td>28920.00</td>\n",
              "      <td>28897.36</td>\n",
              "      <td>28907.59</td>\n",
              "      <td>28896.96</td>\n",
              "      <td>28897.27</td>\n",
              "      <td>28917.81</td>\n",
              "      <td>28880.00</td>\n",
              "      <td>28852.74</td>\n",
              "      <td>28849.92</td>\n",
              "      <td>28820.01</td>\n",
              "      <td>28845.33</td>\n",
              "      <td>28826.30</td>\n",
              "      <td>28850.00</td>\n",
              "      <td>28997.38</td>\n",
              "      <td>28954.67</td>\n",
              "      <td>28966.03</td>\n",
              "      <td>28900.00</td>\n",
              "      <td>28965.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-12-30 23:46:00</th>\n",
              "      <td>28880.00</td>\n",
              "      <td>-19.00</td>\n",
              "      <td>-78.53</td>\n",
              "      <td>28880.00</td>\n",
              "      <td>28838.89</td>\n",
              "      <td>28860.66</td>\n",
              "      <td>28882.50</td>\n",
              "      <td>28866.78</td>\n",
              "      <td>28868.04</td>\n",
              "      <td>28874.00</td>\n",
              "      <td>28890.75</td>\n",
              "      <td>28919.92</td>\n",
              "      <td>28917.27</td>\n",
              "      <td>28899.00</td>\n",
              "      <td>28851.20</td>\n",
              "      <td>28859.08</td>\n",
              "      <td>28846.23</td>\n",
              "      <td>28859.00</td>\n",
              "      <td>28869.79</td>\n",
              "      <td>28835.96</td>\n",
              "      <td>28850.00</td>\n",
              "      <td>28834.94</td>\n",
              "      <td>28839.21</td>\n",
              "      <td>28835.01</td>\n",
              "      <td>28835.01</td>\n",
              "      <td>28864.23</td>\n",
              "      <td>28820.35</td>\n",
              "      <td>28842.28</td>\n",
              "      <td>28809.16</td>\n",
              "      <td>28791.40</td>\n",
              "      <td>28766.83</td>\n",
              "      <td>28720.00</td>\n",
              "      <td>28712.09</td>\n",
              "      <td>28666.83</td>\n",
              "      <td>28686.71</td>\n",
              "      <td>28664.75</td>\n",
              "      <td>28711.00</td>\n",
              "      <td>28697.52</td>\n",
              "      <td>28732.34</td>\n",
              "      <td>28719.98</td>\n",
              "      <td>...</td>\n",
              "      <td>28814.21</td>\n",
              "      <td>28829.90</td>\n",
              "      <td>28765.81</td>\n",
              "      <td>28766.09</td>\n",
              "      <td>28740.21</td>\n",
              "      <td>28750.61</td>\n",
              "      <td>28784.50</td>\n",
              "      <td>28783.40</td>\n",
              "      <td>28752.90</td>\n",
              "      <td>28796.92</td>\n",
              "      <td>28804.12</td>\n",
              "      <td>28877.48</td>\n",
              "      <td>28824.44</td>\n",
              "      <td>28832.33</td>\n",
              "      <td>28794.29</td>\n",
              "      <td>28841.73</td>\n",
              "      <td>28860.21</td>\n",
              "      <td>28860.53</td>\n",
              "      <td>28835.55</td>\n",
              "      <td>28899.17</td>\n",
              "      <td>28889.85</td>\n",
              "      <td>28936.77</td>\n",
              "      <td>28925.78</td>\n",
              "      <td>28920.00</td>\n",
              "      <td>28897.36</td>\n",
              "      <td>28907.59</td>\n",
              "      <td>28896.96</td>\n",
              "      <td>28897.27</td>\n",
              "      <td>28917.81</td>\n",
              "      <td>28880.00</td>\n",
              "      <td>28852.74</td>\n",
              "      <td>28849.92</td>\n",
              "      <td>28820.01</td>\n",
              "      <td>28845.33</td>\n",
              "      <td>28826.30</td>\n",
              "      <td>28850.00</td>\n",
              "      <td>28997.38</td>\n",
              "      <td>28954.67</td>\n",
              "      <td>28966.03</td>\n",
              "      <td>28900.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-12-30 23:47:00</th>\n",
              "      <td>28883.69</td>\n",
              "      <td>-33.58</td>\n",
              "      <td>-54.27</td>\n",
              "      <td>28883.69</td>\n",
              "      <td>28880.00</td>\n",
              "      <td>28838.89</td>\n",
              "      <td>28860.66</td>\n",
              "      <td>28882.50</td>\n",
              "      <td>28866.78</td>\n",
              "      <td>28868.04</td>\n",
              "      <td>28874.00</td>\n",
              "      <td>28890.75</td>\n",
              "      <td>28919.92</td>\n",
              "      <td>28917.27</td>\n",
              "      <td>28899.00</td>\n",
              "      <td>28851.20</td>\n",
              "      <td>28859.08</td>\n",
              "      <td>28846.23</td>\n",
              "      <td>28859.00</td>\n",
              "      <td>28869.79</td>\n",
              "      <td>28835.96</td>\n",
              "      <td>28850.00</td>\n",
              "      <td>28834.94</td>\n",
              "      <td>28839.21</td>\n",
              "      <td>28835.01</td>\n",
              "      <td>28835.01</td>\n",
              "      <td>28864.23</td>\n",
              "      <td>28820.35</td>\n",
              "      <td>28842.28</td>\n",
              "      <td>28809.16</td>\n",
              "      <td>28791.40</td>\n",
              "      <td>28766.83</td>\n",
              "      <td>28720.00</td>\n",
              "      <td>28712.09</td>\n",
              "      <td>28666.83</td>\n",
              "      <td>28686.71</td>\n",
              "      <td>28664.75</td>\n",
              "      <td>28711.00</td>\n",
              "      <td>28697.52</td>\n",
              "      <td>28732.34</td>\n",
              "      <td>...</td>\n",
              "      <td>28818.06</td>\n",
              "      <td>28814.21</td>\n",
              "      <td>28829.90</td>\n",
              "      <td>28765.81</td>\n",
              "      <td>28766.09</td>\n",
              "      <td>28740.21</td>\n",
              "      <td>28750.61</td>\n",
              "      <td>28784.50</td>\n",
              "      <td>28783.40</td>\n",
              "      <td>28752.90</td>\n",
              "      <td>28796.92</td>\n",
              "      <td>28804.12</td>\n",
              "      <td>28877.48</td>\n",
              "      <td>28824.44</td>\n",
              "      <td>28832.33</td>\n",
              "      <td>28794.29</td>\n",
              "      <td>28841.73</td>\n",
              "      <td>28860.21</td>\n",
              "      <td>28860.53</td>\n",
              "      <td>28835.55</td>\n",
              "      <td>28899.17</td>\n",
              "      <td>28889.85</td>\n",
              "      <td>28936.77</td>\n",
              "      <td>28925.78</td>\n",
              "      <td>28920.00</td>\n",
              "      <td>28897.36</td>\n",
              "      <td>28907.59</td>\n",
              "      <td>28896.96</td>\n",
              "      <td>28897.27</td>\n",
              "      <td>28917.81</td>\n",
              "      <td>28880.00</td>\n",
              "      <td>28852.74</td>\n",
              "      <td>28849.92</td>\n",
              "      <td>28820.01</td>\n",
              "      <td>28845.33</td>\n",
              "      <td>28826.30</td>\n",
              "      <td>28850.00</td>\n",
              "      <td>28997.38</td>\n",
              "      <td>28954.67</td>\n",
              "      <td>28966.03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-12-30 23:48:00</th>\n",
              "      <td>28871.54</td>\n",
              "      <td>-48.38</td>\n",
              "      <td>-21.05</td>\n",
              "      <td>28871.54</td>\n",
              "      <td>28883.69</td>\n",
              "      <td>28880.00</td>\n",
              "      <td>28838.89</td>\n",
              "      <td>28860.66</td>\n",
              "      <td>28882.50</td>\n",
              "      <td>28866.78</td>\n",
              "      <td>28868.04</td>\n",
              "      <td>28874.00</td>\n",
              "      <td>28890.75</td>\n",
              "      <td>28919.92</td>\n",
              "      <td>28917.27</td>\n",
              "      <td>28899.00</td>\n",
              "      <td>28851.20</td>\n",
              "      <td>28859.08</td>\n",
              "      <td>28846.23</td>\n",
              "      <td>28859.00</td>\n",
              "      <td>28869.79</td>\n",
              "      <td>28835.96</td>\n",
              "      <td>28850.00</td>\n",
              "      <td>28834.94</td>\n",
              "      <td>28839.21</td>\n",
              "      <td>28835.01</td>\n",
              "      <td>28835.01</td>\n",
              "      <td>28864.23</td>\n",
              "      <td>28820.35</td>\n",
              "      <td>28842.28</td>\n",
              "      <td>28809.16</td>\n",
              "      <td>28791.40</td>\n",
              "      <td>28766.83</td>\n",
              "      <td>28720.00</td>\n",
              "      <td>28712.09</td>\n",
              "      <td>28666.83</td>\n",
              "      <td>28686.71</td>\n",
              "      <td>28664.75</td>\n",
              "      <td>28711.00</td>\n",
              "      <td>28697.52</td>\n",
              "      <td>...</td>\n",
              "      <td>28870.18</td>\n",
              "      <td>28818.06</td>\n",
              "      <td>28814.21</td>\n",
              "      <td>28829.90</td>\n",
              "      <td>28765.81</td>\n",
              "      <td>28766.09</td>\n",
              "      <td>28740.21</td>\n",
              "      <td>28750.61</td>\n",
              "      <td>28784.50</td>\n",
              "      <td>28783.40</td>\n",
              "      <td>28752.90</td>\n",
              "      <td>28796.92</td>\n",
              "      <td>28804.12</td>\n",
              "      <td>28877.48</td>\n",
              "      <td>28824.44</td>\n",
              "      <td>28832.33</td>\n",
              "      <td>28794.29</td>\n",
              "      <td>28841.73</td>\n",
              "      <td>28860.21</td>\n",
              "      <td>28860.53</td>\n",
              "      <td>28835.55</td>\n",
              "      <td>28899.17</td>\n",
              "      <td>28889.85</td>\n",
              "      <td>28936.77</td>\n",
              "      <td>28925.78</td>\n",
              "      <td>28920.00</td>\n",
              "      <td>28897.36</td>\n",
              "      <td>28907.59</td>\n",
              "      <td>28896.96</td>\n",
              "      <td>28897.27</td>\n",
              "      <td>28917.81</td>\n",
              "      <td>28880.00</td>\n",
              "      <td>28852.74</td>\n",
              "      <td>28849.92</td>\n",
              "      <td>28820.01</td>\n",
              "      <td>28845.33</td>\n",
              "      <td>28826.30</td>\n",
              "      <td>28850.00</td>\n",
              "      <td>28997.38</td>\n",
              "      <td>28954.67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-12-30 23:49:00</th>\n",
              "      <td>28844.56</td>\n",
              "      <td>-46.19</td>\n",
              "      <td>65.98</td>\n",
              "      <td>28844.56</td>\n",
              "      <td>28871.54</td>\n",
              "      <td>28883.69</td>\n",
              "      <td>28880.00</td>\n",
              "      <td>28838.89</td>\n",
              "      <td>28860.66</td>\n",
              "      <td>28882.50</td>\n",
              "      <td>28866.78</td>\n",
              "      <td>28868.04</td>\n",
              "      <td>28874.00</td>\n",
              "      <td>28890.75</td>\n",
              "      <td>28919.92</td>\n",
              "      <td>28917.27</td>\n",
              "      <td>28899.00</td>\n",
              "      <td>28851.20</td>\n",
              "      <td>28859.08</td>\n",
              "      <td>28846.23</td>\n",
              "      <td>28859.00</td>\n",
              "      <td>28869.79</td>\n",
              "      <td>28835.96</td>\n",
              "      <td>28850.00</td>\n",
              "      <td>28834.94</td>\n",
              "      <td>28839.21</td>\n",
              "      <td>28835.01</td>\n",
              "      <td>28835.01</td>\n",
              "      <td>28864.23</td>\n",
              "      <td>28820.35</td>\n",
              "      <td>28842.28</td>\n",
              "      <td>28809.16</td>\n",
              "      <td>28791.40</td>\n",
              "      <td>28766.83</td>\n",
              "      <td>28720.00</td>\n",
              "      <td>28712.09</td>\n",
              "      <td>28666.83</td>\n",
              "      <td>28686.71</td>\n",
              "      <td>28664.75</td>\n",
              "      <td>28711.00</td>\n",
              "      <td>...</td>\n",
              "      <td>28885.35</td>\n",
              "      <td>28870.18</td>\n",
              "      <td>28818.06</td>\n",
              "      <td>28814.21</td>\n",
              "      <td>28829.90</td>\n",
              "      <td>28765.81</td>\n",
              "      <td>28766.09</td>\n",
              "      <td>28740.21</td>\n",
              "      <td>28750.61</td>\n",
              "      <td>28784.50</td>\n",
              "      <td>28783.40</td>\n",
              "      <td>28752.90</td>\n",
              "      <td>28796.92</td>\n",
              "      <td>28804.12</td>\n",
              "      <td>28877.48</td>\n",
              "      <td>28824.44</td>\n",
              "      <td>28832.33</td>\n",
              "      <td>28794.29</td>\n",
              "      <td>28841.73</td>\n",
              "      <td>28860.21</td>\n",
              "      <td>28860.53</td>\n",
              "      <td>28835.55</td>\n",
              "      <td>28899.17</td>\n",
              "      <td>28889.85</td>\n",
              "      <td>28936.77</td>\n",
              "      <td>28925.78</td>\n",
              "      <td>28920.00</td>\n",
              "      <td>28897.36</td>\n",
              "      <td>28907.59</td>\n",
              "      <td>28896.96</td>\n",
              "      <td>28897.27</td>\n",
              "      <td>28917.81</td>\n",
              "      <td>28880.00</td>\n",
              "      <td>28852.74</td>\n",
              "      <td>28849.92</td>\n",
              "      <td>28820.01</td>\n",
              "      <td>28845.33</td>\n",
              "      <td>28826.30</td>\n",
              "      <td>28850.00</td>\n",
              "      <td>28997.38</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1929471 rows × 123 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                         Open  diff_Open   t+10  ...     t-117     t-118     t-119\n",
              "Timestamp                                        ...                              \n",
              "2017-05-01 01:59:00   1348.50      -1.55   2.91  ...   1349.49   1352.41   1348.88\n",
              "2017-05-01 02:00:00   1348.51      -1.54   3.38  ...   1350.11   1349.49   1352.41\n",
              "2017-05-01 02:01:00   1345.51      -4.54   5.57  ...   1351.25   1350.11   1349.49\n",
              "2017-05-01 02:02:00   1346.06      -6.89   7.10  ...   1351.24   1351.25   1350.11\n",
              "2017-05-01 02:03:00   1346.06      -6.89   5.48  ...   1349.47   1351.24   1351.25\n",
              "...                       ...        ...    ...  ...       ...       ...       ...\n",
              "2020-12-30 23:45:00  28838.89     -12.31 -29.82  ...  28966.03  28900.00  28965.00\n",
              "2020-12-30 23:46:00  28880.00     -19.00 -78.53  ...  28954.67  28966.03  28900.00\n",
              "2020-12-30 23:47:00  28883.69     -33.58 -54.27  ...  28997.38  28954.67  28966.03\n",
              "2020-12-30 23:48:00  28871.54     -48.38 -21.05  ...  28850.00  28997.38  28954.67\n",
              "2020-12-30 23:49:00  28844.56     -46.19  65.98  ...  28826.30  28850.00  28997.38\n",
              "\n",
              "[1929471 rows x 123 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Z2Apiir0LYh"
      },
      "source": [
        ""
      ],
      "id": "3Z2Apiir0LYh",
      "execution_count": null,
      "outputs": []
    }
  ]
}